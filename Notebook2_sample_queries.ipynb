{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad8a1402-b596-4eab-95b4-e264a33db4ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions a\n",
    "\n",
    "data = [(\"A\", 2), (\"B\", 3), (\"C\", 1)]\n",
    "df = spark.createDataFrame(data, [\"Item_Name\", \"Count\"])\n",
    "\n",
    "result = df.withColumn('seq', F.sequence(F.lit(1), F.col('Count'))).withColumn('Item_Name', F.explode(F.col('seq')))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87d73dce-a204-4f34-b0b4-6ed570bb7a5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [1,2,3]\n",
    "df = spark.createDataFrame(data, [\"Item_No\"])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00583b88-84c9-496e-a6b0-465ffa8e57b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('numbers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e4fa0b-3020-4ebe-b9eb-21fd2d86f77a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select Item_No, 1 as rank from numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64becc41-e45e-47c5-9372-206d3f4e841f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "data = [\n",
    "    ('2026-01-01', 'Success'),\n",
    "    ('2026-01-02', 'Success'),\n",
    "    ('2026-01-03', 'Success'),\n",
    "    ('2026-01-04', 'Fail'),\n",
    "    ('2026-01-05', 'Fail'),\n",
    "    ('2026-01-06', 'Fail'),\n",
    "    ('2026-01-07', 'Success'),\n",
    "    ('2026-01-08', 'Success')\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, ['event_date', 'event_status']).withColumn('event_date', to_date('event_date', 'yyyy-MM-dd'))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d907d79-02dd-406f-8317-2e7e2c9b3a85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bec476b-b528-4e64-a2fb-788fdee66707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "w = Window.orderBy(col('event_date'))\n",
    "r_df = df.withColumn('status_flag', when(col('event_status') != lag(col('event_status')).over(w), 1).otherwise(0)).withColumn('grp', sum(col('status_flag')).over(w))\n",
    "\n",
    "final_df = r_df.groupBy(col('event_status'),'grp').agg(min('event_date').alias('start_date'), max('event_date').alias('end_date')).select('start_date','end_date')\n",
    "\n",
    "final_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b875e4-a747-4e5f-a9b2-3c95b0506054",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769786095871}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "with status_flag as (\n",
    "    select event_date, event_status,\n",
    "    case when event_status != lag(event_status)over(order by event_date) then 1 else 0 end as flag from status\n",
    ")\n",
    ",grouped as (\n",
    "    select event_date, event_status,\n",
    "    sum(flag)over(order by event_date) as grp from status_flag\n",
    ")\n",
    "\n",
    "select min(event_date) as start_date, max(event_date) as end_date from grouped\n",
    "group by event_status, grp\n",
    "order by start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50731397-c850-41e2-b65c-17bc8244fef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with recursive cte as (\n",
    "    select Item_No, 1 as rank from numbers\n",
    "    union all\n",
    "    select Item_No, rank+1 from cte where rank<item_no\n",
    ")\n",
    "select Item_no from cte order by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "703be69c-1800-48cb-bea4-83ff5b91820d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [(\"A\", 2), (\"B\", 3), (\"C\", 1)]\n",
    "df = spark.createDataFrame(data, [\"Item_Name\", \"Count\"])\n",
    "\n",
    "result = (\n",
    "    df\n",
    "    .withColumn(\"seq\", F.sequence(F.lit(1), F.col(\"Count\")))\n",
    "    .withColumn(\"dummy\", F.explode(\"seq\"))\n",
    "    .select(F.col('Item_Name'))\n",
    ")\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efe107a9-530a-4cb5-b16c-b7a419451471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "id | event_date\n",
    "1  | 2024-01-15\n",
    "2  | 2024-03-10\n",
    "3  | 2024-03-25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4417db48-5e55-44cc-afba-4f0788590009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = [(1, '2024-01-15'), (2, '2024-03-10'), (3, '2024-03-25')]\n",
    "df = spark.createDataFrame(data, [\"id\", \"event_date\"])\n",
    "df1 = df.withColumn('event_date', to_date('event_date', 'yyyy-MM-dd'))\n",
    "\n",
    "result_df = df1.withColumn('month', month('event_date')).groupBy('month').agg(count('*').alias('cnt')).select('month','cnt')\n",
    "\n",
    "result_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af393799-8b35-4b13-8e69-c891f9233a51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = [(1, 'raju@gmail.com'), (2, 'angababu'), (3, 'jahd@kdd.com')]\n",
    "df = spark.createDataFrame(data, [\"id\", \"email\"])\n",
    "\n",
    "result_df = df.filter(col('email').rlike('^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z]'))\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "356e0e0c-64a6-403e-9e89-15c699f166ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "with cte as (\n",
    "  select emp_id, dept, salary,\n",
    "  row_number()\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import Window, row_number()\n",
    "\n",
    "window = Window.partitionBy('dept').orderBy('salary',ascending=False)\n",
    "\n",
    "result_df = emp_df.withColumn('rn',row_number().over(window)).filter('rn' == 1).drop('rn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adeb7e63-9950-4982-89c4-fe1cf534bdd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "| order_id | customer_id | order_date | amount |\n",
    "| -------- | ----------- | ---------- | ------ |\n",
    "| 1        | C1          | 2024-01-01 | 100    |\n",
    "| 2        | C1          | 2024-01-10 | 200    |\n",
    "| 3        | C1          | 2024-02-05 | 300    |\n",
    "| 4        | C2          | 2024-01-03 | 150    |\n",
    "| 5        | C2          | 2024-01-20 | 250    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96a6eb33-919d-4742-a56d-ac572d31789d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('order_id', IntegerType(), True),\n",
    "    StructField('customer_id', StringType(), True),\n",
    "    StructField('order_date', StringType(), True),\n",
    "    StructField('amount', IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, 'C1', '2024-01-01', 100),\n",
    "    (2, 'C1', '2024-01-10', 200),\n",
    "    (3, 'C1', '2024-02-05', 300),\n",
    "    (4, 'C2', '2024-01-03', 150),\n",
    "    (5, 'C2', '2024-01-20', 250)\n",
    "]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "customer_df = df.withColumn('order_date', to_date('order_date', 'yyyy-MM-dd'))\n",
    "\n",
    "\n",
    "month_df = customer_df.withColumn('month',date_format(col('order_date'), 'MM'))\n",
    "\n",
    "\n",
    "window = Window.partitionBy(col('customer_id'),col('month')).orderBy(col('order_date'))\n",
    "\n",
    "result_df = month_df.withColumn('rn',row_number().over(window)).filter(col('rn')==1).drop(col('rn')).select('customer_id','order_date','amount')\n",
    "\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4603d863-2ddc-4603-94e9-102c08943cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "month_df.createOrReplaceTempView('month_df')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab977eee-786a-47f9-b794-6773005c4126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "with cte as (\n",
    "  select customer_id, order_date, amount,\n",
    "  row_number() over(partition by customer_id, month order by order_date) as rn\n",
    "  from month_df\n",
    ")\n",
    "select customer_id, order_date, amount from cte where rn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b75132-827f-4260-9659-d880028d8e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from pyspark.sql.window import Window\n",
    "schema = StructType([\n",
    "    StructField('user_id', StringType(), True),\n",
    "    StructField('txn_date', StringType(), True),\n",
    "    StructField('amount', IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    ('U1', '2024-01-01', 100),\n",
    "    ('U1', '2024-01-05', 200),\n",
    "    ('U1', '2024-01-10', 300),\n",
    "    ('U2', '2024-01-03', 150),\n",
    "    ('U2', '2024-01-20', 50)\n",
    "]\n",
    "\n",
    "users = spark.createDataFrame(data, schema)\n",
    "\n",
    "result_df = users.withColumn('cum_sum', sum(col('amount')).over(Window.partitionBy(col('user_id')).orderBy(col('txn_date')))).filter(col('cum_sum')>=250).select('user_id','txn_date','amount').limit(1)\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e69ce7c0-1969-444b-87f0-5d741883c192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d1f15df-8354-4e34-b8d0-d249918890e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "users.createOrReplaceTempView('users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50343f3b-8d06-487b-87e0-4a33e399dbf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93c8f1e8-e166-46e1-8201-e17e1630a958",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769605450699}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "with cte as (\n",
    "select *,\n",
    "sum(amount)over(partition by user_id order by txn_date) as cum_sum\n",
    "from users\n",
    ")\n",
    "select user_id, txn_date, amount from cte where cum_sum>250\n",
    "order by txn_date\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fd00aaa-1fd5-45ac-8b2f-a6226ef4f6b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 16"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "schema = StructType([StructField(\"order_id\", IntegerType()), StructField(\"items\", StringType())])\n",
    "data = [\n",
    "    (1, \"A,B,C\"),\n",
    "    (2, \"B,C\"),\n",
    "    (3, \"A,B\"),\n",
    "    (4, \"C\")\n",
    "]\n",
    "df_orders = spark.createDataFrame(data, [\"order_id\", \"items\"])\n",
    "\n",
    "# Split the items string into an array before exploding\n",
    "final_df = df_orders.withColumn('item', explode(split(col('items'), ',')))\n",
    "\n",
    "result_df = final_df.groupBy('item').agg(count('*').alias('cnt')).select('item','cnt').orderBy('item')\n",
    "\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "825b8380-9f2a-444d-a2d6-ce21d73663bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_orders.createOrReplaceTempView('orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d66c802-d9ec-4bfa-b4e3-3aa9da544a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with cte as (\n",
    "  select *,\n",
    "    split(items, ',') as items_array\n",
    "  from orders\n",
    ")\n",
    "select *\n",
    "from cte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fcff485-4b2d-41b3-860d-be6b8fa865ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = [\n",
    "    ('A', 'Jan', 100),\n",
    "    ('A', 'Feb', 200),\n",
    "    ('B', 'Jan', 150),\n",
    "    ('B', 'Feb', 300),\n",
    "    ('C', 'Jan', 250)\n",
    "]\n",
    "columns = ['product', 'month', 'revenue']\n",
    "df_products = spark.createDataFrame(data, columns)\n",
    "result_df = df_products.groupBy('product').agg(sum(when(col('month')=='Jan', col('revenue')).otherwise(0)).alias('Jan'), sum(when(col('month')=='Feb', col('revenue')).otherwise(0)).alias('Feb'))\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ffc7d7-1875-454a-8a1d-026e13d6332f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = df_products.groupBy('product').agg(sum(when(col('month')=='Jan', col('revenue')).otherwise(0)).alias('Jan'), sum(when(col('month')=='Feb', col('revenue')).otherwise(0)).alias('Feb'))\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98831084-2d78-45e8-ae2a-db349f39e845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "| product | Jan | Feb |\n",
    "| ------- | --- | --- |\n",
    "| A       | 100 | 200 |\n",
    "| B       | 150 | 300 |\n",
    "| C       | 250 | 0   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d02c95d5-9520-4e23-b2c4-218b5f67200f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_products.createOrReplaceTempView('products')\n",
    "display(spark.sql('select * from products'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "539b493d-0d4b-4a4b-b683-6cd9b9b1505a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select product,\n",
    "sum(case when month = 'Jan' then revenue else 0 end) as Jan,\n",
    "sum(case when month = 'Feb' then revenue else 0 end) as Feb\n",
    "from products\n",
    "group by product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdfc555e-a78c-4d70-b823-807295042ee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = [(1, 'Durga-Prasad'), (2, 'Ranga-Babu'), (3, 'Prakash Babu'), (4, 'Murali Babu')]\n",
    "df = spark.createDataFrame(data, [\"id\", \"Name\"])\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19877df8-6fce-4ad4-be1b-8e0fed36366c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, split\n",
    "\n",
    "result_df = df.withColumn(\"First_Name\", split(col('Name'), '[ -]')[0]).withColumn(\"Last_Name\", split(col('Name'),'[ -]')[1])\n",
    "result_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e52224-d41b-4fbc-8c8c-83874b91514a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = [('abc', 'MATH', 98), ('abc', 'phy', 96), ('abc', 'che', 87), ('def', 'che', 98), ('def', 'MATH', 76)]\n",
    "Sub_df = spark.createDataFrame(data, [\"Name\", \"Sub\", \"marks\"])\n",
    "Sub_df.display()\n",
    "Sub_df.createOrReplaceTempView('Subjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5180aacf-1886-42aa-a7ae-a52af91f362b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770478798379}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "final_df = Sub_df.groupBy('Name').pivot('Sub').sum('marks')\n",
    "final_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd3fa13-0db4-41f4-ae4c-6c945fbacc3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from Subjects\n",
    "PIVOT(sum(marks) for Sub in ('MATH','phy','che'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33b342f9-c242-47e7-8efc-0418e340f623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "str = 'peorun hajdytr qazxsw lapdoyrtwnvm lkmjniu 123 azxcvbgdfsteyruik'\n",
    "\n",
    "seen = []\n",
    "for ch in str:\n",
    "    if ch.isalpha() and ch not in seen:\n",
    "        seen.append(ch)\n",
    "\n",
    "print(seen)\n",
    "if len(seen) == 26:\n",
    "    print(\"String is a pangram\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3713a59d-d6e6-40b7-9af6-4b104f58baf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc4aa879-f769-4014-9135-d61047db9578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6540166480048724,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook2_sample_queries",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
