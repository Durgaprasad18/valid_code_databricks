{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d8bb484-8b5a-41ae-bee4-21ecf57ff19d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# End-to-End Ingestion Pipeline\n",
    "# ================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "\n",
    "# --------------------------------\n",
    "# Configurations\n",
    "# --------------------------------\n",
    "raw_path = \"/mnt/raw\"\n",
    "processed_path = \"/mnt/processed\"\n",
    "error_log_path = \"/mnt/error_logs\"\n",
    "watermark_date = \"2024-01-01\"\n",
    "\n",
    "required_columns = [\"cust_id\", \"order_value\", \"order_date\"]\n",
    "\n",
    "expected_schema = StructType([\n",
    "    StructField(\"cust_id\", StringType(), True),\n",
    "    StructField(\"order_value\", DoubleType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "])\n",
    "\n",
    "# --------------------------------\n",
    "# Error Logging\n",
    "# --------------------------------\n",
    "def log_error(file_name, issue, detected_schema=None, severity=\"ERROR\"):\n",
    "    error_data = [(file_name, issue, severity, str(detected_schema), datetime.now())]\n",
    "\n",
    "    (spark.createDataFrame(\n",
    "        error_data,\n",
    "        [\"file_name\", \"issue\", \"severity\", \"detected_schema\", \"logged_at\"]\n",
    "    )\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(error_log_path))\n",
    "\n",
    "# --------------------------------\n",
    "# File Reader\n",
    "# --------------------------------\n",
    "def read_file(file):\n",
    "    if file.name.endswith(\".csv\"):\n",
    "        return (spark.read\n",
    "                .schema(expected_schema)\n",
    "                .option(\"header\", True)\n",
    "                .csv(file.path))\n",
    "    elif file.name.endswith(\".parquet\"):\n",
    "        return spark.read.parquet(file.path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "# --------------------------------\n",
    "# Schema Validation\n",
    "# --------------------------------\n",
    "def validate_schema(df, file_name):\n",
    "\n",
    "    incoming = {c.lower(): df.schema[c].dataType.simpleString() for c in df.columns}\n",
    "    expected = {f.name.lower(): f.dataType.simpleString() for f in expected_schema.fields}\n",
    "\n",
    "    missing = expected.keys() - incoming.keys()\n",
    "    mismatched = {\n",
    "        c: (incoming[c], expected[c])\n",
    "        for c in expected.keys() & incoming.keys()\n",
    "        if incoming[c] != expected[c]\n",
    "    }\n",
    "\n",
    "    if missing or mismatched:\n",
    "        log_error(file_name,\n",
    "                  f\"Schema issues | Missing: {missing} | Mismatched: {mismatched}\",\n",
    "                  df.schema)\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# --------------------------------\n",
    "# Data Quality Validation\n",
    "# --------------------------------\n",
    "def data_quality_check(df, file_name):\n",
    "\n",
    "    result = (\n",
    "        df.select([\n",
    "            F.sum(F.col(c).isNull().cast(\"int\")).alias(c)\n",
    "            for c in required_columns\n",
    "        ])\n",
    "        .first()\n",
    "        .asDict()\n",
    "    )\n",
    "\n",
    "    issues = {c: v for c, v in result.items() if v > 0}\n",
    "\n",
    "    if issues:\n",
    "        log_error(file_name, f\"Null violations: {issues}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# --------------------------------\n",
    "# Main Processing Function\n",
    "# --------------------------------\n",
    "def process_files():\n",
    "\n",
    "    files = dbutils.fs.ls(raw_path)\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            # Step 1: Read file\n",
    "            df = read_file(file)\n",
    "\n",
    "            # Step 2: Schema validation\n",
    "            if not validate_schema(df, file.name):\n",
    "                continue\n",
    "\n",
    "            # Step 3: Data quality check\n",
    "            if not data_quality_check(df, file.name):\n",
    "                continue\n",
    "\n",
    "            # Step 4: Incremental filter\n",
    "            if \"order_date\" not in df.columns:\n",
    "                log_error(file.name,\n",
    "                          \"Missing incremental column 'order_date'\",\n",
    "                          df.columns)\n",
    "                continue\n",
    "\n",
    "            df_incremental = df.filter(\n",
    "                F.col(\"order_date\") > F.lit(watermark_date)\n",
    "            )\n",
    "\n",
    "            if df_incremental.rdd.isEmpty():\n",
    "                continue\n",
    "\n",
    "            # Step 5: Deduplication\n",
    "            df_dedup = df_incremental.dropDuplicates([\"cust_id\", \"order_date\"])\n",
    "\n",
    "            # Step 6: Write to Delta (Schema Evolution Enabled)\n",
    "            if DeltaTable.isDeltaTable(spark, processed_path):\n",
    "                (df_dedup.write\n",
    "                 .format(\"delta\")\n",
    "                 .mode(\"append\")\n",
    "                 .option(\"mergeSchema\", \"true\")\n",
    "                 .save(processed_path))\n",
    "            else:\n",
    "                (df_dedup.write\n",
    "                 .format(\"delta\")\n",
    "                 .mode(\"overwrite\")\n",
    "                 .option(\"mergeSchema\", \"true\")\n",
    "                 .save(processed_path))\n",
    "\n",
    "            print(f\"âœ” Successfully processed {file.name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log_error(file.name, str(e))\n",
    "\n",
    "# --------------------------------\n",
    "# Execute Pipeline\n",
    "# --------------------------------\n",
    "process_files()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "End-to-End Datapipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
