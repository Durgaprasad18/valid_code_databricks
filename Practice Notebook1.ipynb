{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04310bca-63c3-44a2-96ad-7e6b8d19f61f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8c00bf-641e-4a6e-a506-358faf08f906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session (if not already created)\n",
    "spark = SparkSession.builder.appName(\"StocksDF\").getOrCreate()\n",
    "\n",
    "# Data from the image\n",
    "data = [\n",
    "    (\"Apple\", \"Buy\", 1, 1500),\n",
    "    (\"Tesla\", \"Buy\", 2, 1200),\n",
    "    (\"Apple\", \"Sell\", 5, 5000),\n",
    "    (\"Samsung\", \"Buy\", 17, 20000),\n",
    "    (\"Tesla\", \"Sell\", 3, 1300),\n",
    "    (\"Tesla\", \"Buy\", 4, 1500),\n",
    "    (\"Tesla\", \"Sell\", 5, 1100),\n",
    "    (\"Tesla\", \"Buy\", 6, 1400),\n",
    "    (\"Samsung\", \"Sell\", 29, 15000),\n",
    "    (\"Tesla\", \"Sell\", 10, 1200)\n",
    "]\n",
    "\n",
    "# Column names\n",
    "columns = [\"stock_name\", \"operation\", \"operation_day\", \"price\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show DataFrame\n",
    "df.createOrReplaceTempView(\"stocks\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ac217b-7312-404d-a7b3-cd4d3a7e02cf",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771414557732}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with cte as (\n",
    "  select stock_name,\n",
    "  sum(case when operation = 'Buy' then price end) as Buy_price,\n",
    "  sum(case when operation = 'Sell' then price end) as Sell_price from stocks\n",
    "  group by stock_name\n",
    ")\n",
    "select stock_name, (Sell_price - Buy_price) as capital_gain_loss from cte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf8a87b-853d-4b40-b1d4-7a54c853fe24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create Spark session (if not already created)\n",
    "spark = SparkSession.builder.appName(\"EventDF\").getOrCreate()\n",
    "\n",
    "# Data from image\n",
    "data = [\n",
    "    (\"fail\", \"2020-01-04\"),\n",
    "    (\"success\", \"2020-01-01\"),\n",
    "    (\"success\", \"2020-01-03\"),\n",
    "    (\"success\", \"2020-01-06\"),\n",
    "    (\"fail\", \"2020-01-05\"),\n",
    "    (\"success\", \"2020-01-02\")\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"event\", StringType(), True),\n",
    "    StructField(\"event_dt\", StringType(), True)  # Initially string\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Convert to Date type (recommended in real projects)\n",
    "df = df.withColumn(\"event_dt\", to_date(\"event_dt\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# Show result\n",
    "df.display()\n",
    "df.createOrReplaceTempView('events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cdc88c4-671b-4c8a-96d7-98930ed2c29d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771419052122}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with cte as (\n",
    "select *,\n",
    "date_diff(event_dt, min(event_dt)over(order by event_dt)) - row_number() over (partition by event order by event_dt)as n,\n",
    "date_add(event_dt, -(row_number() over (partition by event order by event_dt))) as grp\n",
    " from events \n",
    ")\n",
    "select * from cte\n",
    "-- select event, min(event_dt) as start_date, max(event_dt) as end_date from cte\n",
    "-- group by event, grp\n",
    "-- order by start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7349d227-83e6-4a13-8f69-a5f345c99200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Create Spark session (if not already created)\n",
    "spark = SparkSession.builder.appName(\"TeamsDF\").getOrCreate()\n",
    "\n",
    "# Data from image\n",
    "data = [\n",
    "    (\"India\", \"Australia\", \"India\"),\n",
    "    (\"India\", \"England\", \"England\"),\n",
    "    (\"SouthAfrica\", \"India\", \"India\"),\n",
    "    (\"Australia\", \"England\", None),   # NULL value\n",
    "    (\"England\", \"SouthAfrica\", \"SouthAfrica\"),\n",
    "    (\"Australia\", \"India\", \"Australia\")\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"Team_1\", StringType(), True),\n",
    "    StructField(\"Team_2\", StringType(), True),\n",
    "    StructField(\"Result\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.display()\n",
    "df.createOrReplaceTempView('teams')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5bad3ff-737a-4780-b2c3-c80d6c31efa8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771420995513}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select Matches, Matches_won, Matches_lost, (Matches - (Matches_won + Matches_lost)) as Matches_tied from (\n",
    "select team, sum(Matches) as Matches, sum(Matches_won) as Matches_won, sum(Matches_lost) as Matches_lost from (\n",
    "select Team_1 as team, count(*) as Matches,\n",
    "sum(case when Result = Team_1 then 1 else 0 end) as Matches_won,\n",
    "sum(case when Result != Team_1 then 1 else 0 end) as Matches_lost\n",
    "from teams\n",
    " group by 1\n",
    "union all\n",
    "select Team_2 as team, count(*) as Matches,\n",
    "sum(case when Result = Team_2 then 1 else 0 end) as Matches_won ,\n",
    "sum(case when Result != Team_2 then 1 else 0 end) as Matches_lost \n",
    "from teams\n",
    "group by 1\n",
    ")a\n",
    "group by 1\n",
    ")b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13b766aa-f070-4ad7-9cd4-b349ccaa21f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "students = {\"Ram\": \"+91 9988776655\", \"Sita\": \"+92 7766554422\", \"John\": \"+88 654567898\", \"Anjali\": \"+1 9598762345\"}\n",
    "\n",
    "data = [{\"name\": name, \"mobile_no\": mobile_no} for name, mobile_no in students.items()]\n",
    "df = spark.createDataFrame(data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf055a4-fc33-48d8-894a-b21d85c25ed7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('students')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73246c66-c7cd-4405-840b-767cb9f8d05d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *, concat(split(mobile_no, ' ')[0], repeat('*', length(split(mobile_no, ' ')[1])-4), right(split(mobile_no, ' ')[1],4) ) as new_phn from students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa6d609-dae5-48ea-b09f-d12bee365c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1672f688-0c5a-443c-8d3c-36eda3a4ce91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = df.withColumn('new_phn', concat(split(col('mobile_no'), ' ')[0], repeat('*', length(split(col('mobile_no'), ' ')[1])-4), right(split(col('mobile_no'), ' ')[1],4) ))\n",
    "final_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6167f145-e02f-4d73-a2ea-c4005c3a4115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Split the mobile number into an array [country_code, number]\n",
    "# 2. Calculate the length of the number part\n",
    "# 3. Mask the middle of the number part\n",
    "# 4. Concatenate it back together\n",
    "\n",
    "df_new = df.withColumn(\"split_phn\", F.split(F.col(\"mobile_no\"), \" \")) \\\n",
    "           .withColumn(\"country_code\", F.col(\"split_phn\").getItem(0)) \\\n",
    "           .withColumn(\"phone_part\", F.col(\"split_phn\").getItem(1)) \\\n",
    "           .withColumn(\"mask_length\", F.length(F.col(\"phone_part\")) - 4) \\\n",
    "           .withColumn(\"new_phn\", F.concat(\n",
    "               F.col(\"country_code\"),\n",
    "               F.repeat(F.lit(\"*\"), F.col(\"mask_length\")),\n",
    "               F.substring(F.col(\"phone_part\"), -4, 4)\n",
    "           )) \\\n",
    "           .drop(\"split_phn\", \"country_code\", \"phone_part\", \"mask_length\")\n",
    "\n",
    "df_new.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "219a21bf-a508-4525-9fff-9586e6b08b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [\n",
    "    # Employee 1 → Has 3 consecutive days\n",
    "    (1, \"2025-01-20\"),\n",
    "    (1, \"2025-01-21\"),\n",
    "    (1, \"2025-01-22\"),\n",
    "    (1, \"2025-01-24\"),\n",
    "\n",
    "    # Employee 2 → No 3 consecutive days\n",
    "    (2, \"2025-01-15\"),\n",
    "    (2, \"2025-01-16\"),\n",
    "    (2, \"2025-01-18\"),\n",
    "\n",
    "    # Employee 3 → Has 3 consecutive days\n",
    "    (3, \"2025-01-10\"),\n",
    "    (3, \"2025-01-11\"),\n",
    "    (3, \"2025-01-12\"),\n",
    "\n",
    "    # Employee 4 → Has 4 consecutive days\n",
    "    (4, \"2025-01-05\"),\n",
    "    (4, \"2025-01-06\"),\n",
    "    (4, \"2025-01-07\"),\n",
    "    (4, \"2025-01-08\"),\n",
    "\n",
    "    # Employee 5 → Only 2 consecutive days\n",
    "    (5, \"2025-01-01\"),\n",
    "    (5, \"2025-01-02\"),\n",
    "\n",
    "    # Employee 6 → Multiple streak attempts\n",
    "    (6, \"2025-01-01\"),\n",
    "    (6, \"2025-01-03\"),\n",
    "    (6, \"2025-01-04\"),\n",
    "    (6, \"2025-01-05\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"login_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "login_df = spark.createDataFrame(data, [\"emp_id\", \"login_date\"]) \\\n",
    "          .withColumn(\"login_date\", to_date(\"login_date\"))\n",
    "\n",
    "login_df.display()\n",
    "login_df.createOrReplaceTempView(\"employee_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c60c5a9b-4035-412f-b36e-d2fc7d84acd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "wn = Window.partitionBy(col('emp_id')).orderBy(col('login_date')) \n",
    "\n",
    "final_df = login_df.withColumn('rn', date_sub(col('login_date'), row_number().over(wn))).groupBy('emp_id','rn').agg(count('*').alias('cnt')).filter(\"cnt >= 3\").select('emp_id', 'rn')\n",
    "\n",
    "final_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ec8bf3-f525-449d-aa32-5fda5d2ffbbe",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770909759857}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with cte as (\n",
    "select emp_id, login_date,\n",
    "date_add(DAY, -row_number()over(partition by emp_id order by login_date), login_date) as rn\n",
    "from employee_logs\n",
    ")\n",
    "select * from cte\n",
    "-- select emp_id, rn from cte\n",
    "-- group by emp_id, rn\n",
    "-- having count(*) >=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da23b7c-20f9-4c9f-8bec-a910356eb0ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, concat, repeat, length, right, lit\n",
    "\n",
    "# split_col = split(col(\"mobile_no\"), \" \")\n",
    "\n",
    "# masked_length = length(split(col(\"mobile_no\"), \" \")[1]) - 4\n",
    "\n",
    "final_df = df.withColumn(\n",
    "    \"new_phn\",\n",
    "    concat(\n",
    "        split(col(\"mobile_no\"), \" \")[0],\n",
    "        repeat(lit(\"*\"), length(split(col(\"mobile_no\"), \" \")[1]) - 4),\n",
    "        substring(split(col(\"mobile_no\"), \" \")[1], -4, 4)\n",
    "    )\n",
    ")\n",
    "\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ddb1eb-1aff-4e33-bb55-21393653ec22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, concat, repeat, length, substring, when, greatest\n",
    "\n",
    "# Split only once\n",
    "# split_col = split(col(\"mobile_no\"), \" \")\n",
    "\n",
    "masked_length = length(split(col(\"mobile_no\"), \" \")[1]) - 4\n",
    "\n",
    "final_df = df.withColumn(\n",
    "    \"new_phn\",\n",
    "    concat(\n",
    "        split(col(\"mobile_no\"), \" \")[0],\n",
    "        repeat(col(\"*\"), masked_length),  # <-- FIX HERE\n",
    "        substring(split(col(\"mobile_no\"), \" \")[1], -4, 4)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "display(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e9f6fde-c9fe-4b41-8f1a-a8a0dcba762d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = df.withColumn('new_phn', concat(split(col('mobile_no'), ' ')[0], repeat('*', length(split(col('mobile_no'), ' ')[1])-4), right(split(col('mobile_no'), ' ')[1],4) ))\n",
    "final_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "808ef770-cb83-4d3d-89e4-dde3efdf5aaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"id\": 1},\n",
    "    {\"id\": 2},\n",
    "    {\"id\": 3},\n",
    "    {\"id\": 4}\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d1f07c-7e10-4e20-9cf8-00cf756ca27f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "result_df = df.withColumn('dummy', sequence(lit(1), col('id')))\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a67a58-e9b1-4e69-98ff-cadedfcadd09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"id\": 1, \"emp_name\": \"CEO\", \"mgr_id\": None},\n",
    "    {\"id\": 2, \"emp_name\": \"CFO\", \"mgr_id\": 1},\n",
    "    {\"id\": 3, \"emp_name\": \"CTO\", \"mgr_id\": 1},\n",
    "    {\"id\": 4, \"emp_name\": \"Manager1\", \"mgr_id\": 2},\n",
    "    {\"id\": 5, \"emp_name\": \"Manager2\", \"mgr_id\": 2},\n",
    "    {\"id\": 6, \"emp_name\": \"Lead\", \"mgr_id\": 4},\n",
    "    {\"id\": 7, \"emp_name\": \"Engineer\", \"mgr_id\": 6}\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bf2d419-d643-46d9-8c99-113cd6792f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.schema['emp_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5145e182-7620-475e-a75a-2c537d92581a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('emp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "438ee62f-0f39-442c-b281-e2f7b3afea79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "with recursive cte as (\n",
    "  select emp_name, id, mgr_id, 1 as emp_level from emp where mgr_id is null\n",
    "  union all\n",
    "  select e.emp_name, e.id, e.mgr_id, m.emp_level+1 as emp_level from emp e join cte m on e.mgr_id = m.id\n",
    ")\n",
    "select emp_name, id, emp_level from cte\n",
    "order by emp_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5fc5268-b69e-47d1-b409-013d7005f705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"col1\": \"A\", \"col2\":\"B\"},\n",
    "    {\"col1\": \"C\", \"col2\":\"D\"},\n",
    "    {\"col1\": \"E\", \"col2\":\"F\"},\n",
    "    {\"col1\": \"G\", \"col2\":\"H\"},\n",
    "    {\"col1\": \"I\", \"col2\":\"J\"}\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "648270f8-d9b4-48eb-855e-32c98439e197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('seating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06870c79-480e-4968-8434-15cd6dda6efa",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770093027268}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "with cte as (\n",
    "  select col2 as col1, col1 as col2,\n",
    "  row_number() over (order by col1, col2) as rn\n",
    "  from seating\n",
    ")\n",
    "select col1, col2,\n",
    "case \n",
    "when rn%2 = 1 and rn = (select max(rn) from cte) then rn\n",
    "when rn%2 = 1 then rn+1\n",
    "else rn-1 end as new_rn\n",
    " from cte\n",
    " order by new_rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70dccc84-f6f0-4ca9-9818-f1d710d2077a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771252471625}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"CreateDataFrameFromImage\").getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"team\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data extracted from image\n",
    "data = [\n",
    "'CSK',\n",
    "'RCB',\n",
    "'MI',\n",
    "'KKR'\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show result\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c61abe5f-ad42-4913-9b43-add3e5161625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with cte as (\n",
    "select NUMB,\n",
    "lead(NUMB,1) OVER(order by SN) as next_id,\n",
    "lead(NUMB,2) OVER(order by SN) as next_next_id from serial\n",
    ")\n",
    "select NUMB from cte \n",
    "where NUMB != next_id and NUMB = next_next_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "109100c7-e4b6-44db-a631-7bfcb2d0255d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('ipl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666ac340-f736-4c19-8440-1f4a798712c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from ipl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c29c4f0-7c10-40e3-95d2-a1089bd6cbce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select concat(team1,'vs',team2) from (\n",
    "select a.team as team1, b.team as team2 from\n",
    "ipl a join ipl b on a.team<>b.team\n",
    ")a\n",
    "where team1<team2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd2a976-a72a-4a6a-baf2-7aae8b113c42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "original_dict = original_dict = {\n",
    "    'a': 1,\n",
    "    'b': 2,\n",
    "    'c': 1,\n",
    "    'd': 3\n",
    "}\n",
    "\n",
    "def reverse_dict(d):\n",
    "    result = {}\n",
    "    for k,v in d.items():\n",
    "        if v in result:\n",
    "            result[v].append(k)\n",
    "        else:\n",
    "            result[v] = [k]\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "print(reverse_dict(original_dict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988b4d39-51fe-4885-af09-06ac9489cacf",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771327427203}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number, datediff, min\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"u1\", \"2023-01-01\"),\n",
    "    (\"u1\", \"2023-01-02\"),\n",
    "    (\"u1\", \"2023-01-04\"),\n",
    "    (\"u1\", \"2023-01-05\"),\n",
    "    (\"u1\", \"2023-01-06\"),\n",
    "    (\"u2\", \"2023-01-10\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"user_id\", \"login_date\"]) \\\n",
    "          .withColumn(\"login_date\", col(\"login_date\").cast(\"date\"))\n",
    "\n",
    "# Step 1: Add row_number per user ordered by date\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"login_date\")\n",
    "\n",
    "df = df.withColumn(\"rn\", row_number().over(window_spec))\n",
    "\n",
    "# Step 2: Create grouping key using date - row_number\n",
    "df = df.withColumn(\n",
    "    \"grp\",\n",
    "    datediff(col(\"login_date\"), min(\"login_date\").over(window_spec)) - col(\"rn\"),\n",
    ").withColumn('new', datediff(col(\"login_date\"), min(\"login_date\").over(window_spec))).withColumn('n_date', min(\"login_date\").over(window_spec))\n",
    "display(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d66cdbf8-ea1f-4aa4-9515-c40b82b4ff96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Count streak length per group\n",
    "streak_window = Window.partitionBy(\"user_id\", \"grp\") \\\n",
    "                       .orderBy(\"login_date\")\n",
    "\n",
    "df = df.withColumn(\"streak\", row_number().over(streak_window))\n",
    "\n",
    "df.display()\n",
    "# # Final result\n",
    "# result = df.select(\"user_id\", \"login_date\", \"streak\") \\\n",
    "#            .orderBy(\"user_id\", \"login_date\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bafb69be-3e23-4a91-872b-97f04797a8ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import *\n",
    "# Create Spark Session (skip if already created)\n",
    "spark = SparkSession.builder.appName(\"CreateProductDF\").getOrCreate()\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"product_name\", StringType(), False),\n",
    "    StructField(\"category\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Data from image\n",
    "data = [\n",
    "    (4, \"Headphones\", \"Accessories\"),\n",
    "    (5, \"Smartwatch\", \"Accessories\"),\n",
    "    (6, \"Keyboard\", \"Accessories\"),\n",
    "    (7, \"Mouse\", \"Accessories\"),\n",
    "    (8, \"Monitor\", \"Accessories\"),\n",
    "    (1, \"Laptop\", \"Electronics\"),\n",
    "    (2, \"Smartphone\", \"Electronics\"),\n",
    "    (3, \"Tablet\", \"Electronics\"),\n",
    "    (9, \"Printer\", \"Electronics\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.display()\n",
    "df.createOrReplaceTempView('items')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ee4d7d-1fd5-4e5e-bec0-920186546147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *, \n",
    "row_number() over(partition by category order by product_id) as rn from items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b6ae10c-1438-4830-a65d-b670447b45d8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771507735951}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with cte1 as (\n",
    "select *, \n",
    "row_number() over(partition by category order by product_id) as rn from items\n",
    "),\n",
    "cte2 as (\n",
    "select *, \n",
    "row_number() over(partition by category order by product_id desc) as rn from items\n",
    ")\n",
    "select c2.product_id as product_id, c1.product_name as product_name, c2.category as category from cte1 c1 join cte2 c2 on c1.category = c2.category and c1.rn = c2.rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b5f7e0-e915-4616-98fa-a75afb500355",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Create Spark Session (skip if already created)\n",
    "spark = SparkSession.builder.appName(\"CityPincodeDF\").getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"city_pincode\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Data from image\n",
    "data = [\n",
    "    (\"Mumbai400001\",),\n",
    "    (\"Delhi110001\",),\n",
    "    (\"Bengaluru560001\",),\n",
    "    (\"Kolkata700001\",),\n",
    "    (\"Chennai600001\",),\n",
    "    (\"Hyderabad500001\",),\n",
    "    (\"Ahmedabad380001\",),\n",
    "    (\"Pune411001\",),\n",
    "    (\"Jaipur302001\",),\n",
    "    (\"Lucknow226001\",)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.display()\n",
    "\n",
    "df.createOrReplaceTempView('pincode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0958675-731c-44a8-87a6-530145505a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *,\n",
    "regexp_replace(city_pincode, '[^A-za-z]', '') as City,\n",
    "regexp_replace(city_pincode, '[^0-9]', '') as Pincode,\n",
    "regexp_substr(city_pincode, '^[A-Za-z]+') as City1,\n",
    "regexp_substr(city_pincode, '[0-9]+') as pin1,\n",
    "regexp_extract(city_pincode, '^[A-Za-z]+', 0) as City2,\n",
    "regexp_extract(city_pincode, '[0-9]+', 0) as pin2\n",
    "from pincode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f67502-d8b0-49c7-b2e4-1668bf99db1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW transactions AS\n",
    "SELECT * FROM (\n",
    "  VALUES\n",
    "    ('2025-06-01',602),('2025-06-02',935),('2025-06-02',205),('2025-06-03',1360),('2025-06-04',770),\n",
    "    ('2025-06-05',606),('2025-06-06',571),('2025-06-07',1200),('2025-06-08',520),\n",
    "    ('2025-06-09',1114),('2025-06-10',621),('2025-06-11',1373),('2025-06-12',1493),\n",
    "    ('2025-06-13',1380),('2025-06-14',1449),('2025-06-15',962),('2025-06-16',793),\n",
    "    ('2025-06-17',1410),('2025-06-18',704),('2025-06-19',1470),('2025-06-20',1031),\n",
    "    ('2025-06-21',1186),('2025-06-22',671),('2025-06-23',1307),('2025-06-24',1198),\n",
    "    ('2025-06-25',615),('2025-06-26',1212),('2025-06-27',1122),('2025-06-28',920),\n",
    "    ('2025-06-29',919),('2025-06-30',1394), ('2025-06-30',1302),('2025-07-01',1123),('2025-07-02',1073),\n",
    "    ('2025-07-03',1021),('2025-07-04',1438),('2025-07-05',1324),('2025-07-06',764),\n",
    "    ('2025-07-07',1431),('2025-07-08',1280),('2025-07-09',1444),('2025-07-10',1130),\n",
    "    ('2025-07-11',1409),('2025-07-12',656),('2025-07-13',643),('2025-07-14',905),\n",
    "    ('2025-07-15',1332),('2025-07-16',894),('2025-07-17',1434),('2025-07-18',719),\n",
    "    ('2025-07-19',787),('2025-07-20',824),('2025-07-21',1467),('2025-07-22',1111),\n",
    "    ('2025-07-23',1060),('2025-07-24',720),('2025-07-25',1296),('2025-07-26',823),\n",
    "    ('2025-07-27',1480),('2025-07-28',879),('2025-07-29',1341),('2025-07-30',1037),\n",
    "    ('2025-07-31',1221),('2025-08-01',916),('2025-08-02',606),('2025-08-03',1094),\n",
    "    ('2025-08-04',1045),('2025-08-05',1192), ('2025-08-05',1000),('2025-08-06',1214),('2025-08-07',1343),\n",
    "    ('2025-08-08',792),('2025-08-09',859),('2025-08-10',1141),('2025-08-11',673),\n",
    "    ('2025-08-12',1042),('2025-08-13',1480),('2025-08-14',1368),('2025-08-15',1195),\n",
    "    ('2025-08-16',1493),('2025-08-17',1434),('2025-08-18',1248),('2025-08-19',1154),\n",
    "    ('2025-08-20',1322),('2025-08-21',680),('2025-08-22',1163),('2025-08-23',901),\n",
    "    ('2025-08-24',923),('2025-08-25',1410),('2025-08-26',608),('2025-08-27',502),\n",
    "    ('2025-08-28',642),('2025-08-29',1188)\n",
    ") AS t(txn_date, amount);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caebe6fe-4b39-4737-a0cc-c6210890b245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with cte as (\n",
    "select txn_date, sum(amount) as total from transactions group by 1),\n",
    "cte2 as (\n",
    "select *,\n",
    "sum(total)over(order by txn_date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) AS 30_day_rolling_sum\n",
    "from cte\n",
    ")\n",
    "select *,\n",
    "coalesce(round(100 * (30_day_rolling_sum - lag(30_day_rolling_sum)over(order by txn_date))/ (lag(30_day_rolling_sum)over(order by txn_date)),2), 0) as percent_change \n",
    "from cte2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bc92cc4-9020-4d9e-8113-32a986fd4214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def even_odd(lst):\n",
    "    even_count = sum(1 for x in lst if x%2 == 0)\n",
    "    odd_count = len(lst) - even_count\n",
    "\n",
    "    return even_count == odd_count\n",
    "\n",
    "\n",
    "print(even_odd([1,2,3,4,5,6,7,8]))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7350298407251683,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Practice Notebook1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
